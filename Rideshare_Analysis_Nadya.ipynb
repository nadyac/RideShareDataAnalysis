{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyft & Uber Rideshare Data Analysis\n",
    "### By Nadya  Pena\n",
    "\n",
    "The following is an example of routine outlier or anomaly detection in data using simple data analytics and statistics. \n",
    "The dataset we will be using is a sample of Uber and Lyft rideshare information.\n",
    "\n",
    "The first few col schema of the data are as follows:\n",
    "\n",
    "\n",
    "|field|type   |\n",
    "| --- | --- |\n",
    "|merchant_name|string |\n",
    "|order_number|string |\n",
    "|user_id| string|\n",
    "|order_time|timestamp|\n",
    "|email_time|timestamp|\n",
    "|insert_time|timestamp|\n",
    "|update_time|timestamp|\n",
    "|order_total_amount|double|\n",
    "|order_points|string|\n",
    "|order_shipping|string|\n",
    "|order_tax|double|\n",
    "|order_subtotal|double|\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Steps of analysis:\n",
    "1. We'll start by loading our data (whether from a table or a file) and we'll read it into a Spark Dataframe.\n",
    "2. Then we'll do some high-level data quality checks and note things like missing values, duplicate records, value distributions, and anything else remarkable.\n",
    "3. Once we've done that, we can look closer at individual fields to see what their values are like. \n",
    "4. Finally, do more specific analytics like looking at average ride counts and ride length per weekday. Use this average to compare ride counts and ride lengths. We can use Z-scores to determine if particular rides or ride counts are unusual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Spark session creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import datetime\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.master(\"local\").appName(\"RideShare\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\n",
    "hiveContext = HiveContext(sc)\n",
    "SQLContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data \n",
    "In this case we'll be reading from a CSV but if you're not reading from a CSV you can read from a DB as well using a spark-based driver for whatever db you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv') \\\n",
    "        .options(header='true', inferSchema='true') \\\n",
    "        .load(r\"C:\\Users\\nadya\\Documents\\RideShareProject\\Data_Rideshare.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Data Quality Checks\n",
    "The following few cells are simply checking for:\n",
    "1. Missing Data\n",
    "2. Duplicate Records\n",
    "3. Number of Records add up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascertain the number of distinct merchants\n",
    "distinct_merchants = df.select(df.merchant_name).distinct()\n",
    "distinct_merchants.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all records for each service\n",
    "print(\"Total number of records :\", df.count())\n",
    "print(\"Total Lyft records: \", df.filter(df.merchant_name == 'Lyft').count())\n",
    "print(\"Total Uber records: \", df.filter(df.merchant_name == 'Uber').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temp table from the Data Frame.\n",
    "df.registerTempTable(\"Rideshares\")\n",
    "result = hiveContext.sql(\"SELECT COUNT(*) AS records FROM Rideshares\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Redundancy check (duplicate records)\n",
    "If duplicate records exist, ascertain that they are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distinct records \n",
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Missingness Check\n",
    "1. Get the number of records with null values. Investigate nulls to see if they are valid.\n",
    "\n",
    "2. Calculate proportion of missing data per field. \n",
    "    * Fields with high proportion of missing values may not be useful for analysis. \n",
    "    * Could be potential anomalies if a field that is usually populated turns up blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of records that have at least 1 missing field\n",
    "\n",
    "query = \"SELECT * \\\n",
    "         FROM Rideshares \\\n",
    "         WHERE merchant_name IS NULL OR \\\n",
    "               user_id IS NULL OR \\\n",
    "               order_number IS NULL OR \\\n",
    "               order_time IS NULL OR \\\n",
    "               email_time IS NULL OR \\\n",
    "               insert_time IS NULL OR \\\n",
    "               update_time IS NULL OR \\\n",
    "               order_total_amount IS NULL OR \\\n",
    "               order_points IS NULL OR \\\n",
    "               order_shipping IS NULL OR \\\n",
    "               order_tax IS NULL OR \\\n",
    "               order_subtotal IS NULL OR \\\n",
    "               order_total_qty IS NULL OR \\\n",
    "               product_description IS NULL OR \\\n",
    "               product_subtitle IS NULL OR \\\n",
    "               item_quantity IS NULL OR \\\n",
    "               item_price IS NULL OR \\\n",
    "               digital_transaction IS NULL OR \\\n",
    "               checksum IS NULL OR \\\n",
    "               product_reseller IS NULL OR \\\n",
    "               product_category IS NULL OR \\\n",
    "               order_discount IS NULL OR \\\n",
    "               SKU IS NULL OR \\\n",
    "               item_id IS NULL OR \\\n",
    "               order_pickup IS NULL OR \\\n",
    "               from_domain IS NULL OR \\\n",
    "               email_subject IS NULL OR \\\n",
    "               delivery_date IS NULL OR \\\n",
    "               start_source_folder_date IS NULL OR \\\n",
    "               end_source_folder_date IS NULL OR \\\n",
    "               file_id IS NULL OR \\\n",
    "               source_dttimestamp IS NULL OR \\\n",
    "               dttimestamp IS NULL\"\n",
    "result = hiveContext.sql(query)\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proportion of missingness for each column\n",
    "# columns with high % of NULLS may not be useful for analysis\n",
    "count = df.count()\n",
    "print(count)\n",
    "for col in df.columns:\n",
    "    query = \"SELECT \\\n",
    "            CAST( \\\n",
    "                SUM( \\\n",
    "                    CASE WHEN {0} IS NULL THEN 1 ELSE 0 END) \\\n",
    "             AS DECIMAL(10,2))/ COUNT(*) AS missing_{1}, \\\n",
    "             SUM(CASE WHEN {0} IS NULL THEN 1 ELSE 0 END) AS how_many_null \\\n",
    "            FROM Rideshares\".format(col, col)\n",
    "    result = hiveContext.sql(query)\n",
    "    result.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look records where order_time is null. Are those valid? perhaps. \n",
    "# Seems these could be other activities related to tipping, corrections, and payment. \n",
    "query= \"SELECT \\\n",
    "        user_id, \\\n",
    "        date(order_time), \\\n",
    "        email_subject, \\\n",
    "        order_total_amount \\\n",
    "        FROM Rideshares \\\n",
    "        WHERE date(order_time) IS NULL \\\n",
    "        ORDER BY date(order_time)\"\n",
    "result = hiveContext.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"SELECT \\\n",
    "        merchant_name, \\\n",
    "        email_subject, \\\n",
    "        order_total_amount \\\n",
    "        FROM Rideshares \\\n",
    "        WHERE date(order_time) IS NOT NULL AND merchant_name = 'Uber'\\\n",
    "        ORDER BY date(order_time)\"\n",
    "result = hiveContext.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Values Exploration\n",
    "Try to understand the expected \"look\" of the data\n",
    "> (i.e. do some fields always have same value, or should the field look different for all records?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of distinct values for fields. \n",
    "# If the field has same value for all records, it might not be very useful for anomaly detection. \n",
    "# Ex: the field item_quantity and order_total_qty are the same for all records\n",
    "fields = [ 'item_quantity', 'order_total_qty',]\n",
    "for col in fields:\n",
    "    query = \"SELECT \\\n",
    "            DISTINCT({0}) \\\n",
    "            FROM Rideshares\".format(col)\n",
    "    result = hiveContext.sql(query)\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Evaluation of Time fields\n",
    "* Time related queries to determine when each time field starts and ends</dt>\n",
    "* note difference between time fields to see how they relate to eachother</dt>\n",
    "* Look at time series trends later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the starting date for each of the date fields\n",
    "cols = [ 'source_dttimestamp','dttimestamp', 'order_time','email_time','insert_time','update_time',]\n",
    "for col in cols:\n",
    "    query = \"SELECT \\\n",
    "                 MIN({0}), \\\n",
    "                 MAX({0}) \\\n",
    "                 FROM Rideshares\".format(col)\n",
    "    result = hiveContext.sql(query)\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the starting date for each of the date fields\n",
    "cols = [ 'source_dttimestamp','dttimestamp', 'order_time','email_time','insert_time','update_time',]\n",
    "\n",
    "for col in cols:\n",
    "    query = \"SELECT \\\n",
    "             date({0}) AS {0}_col, \\\n",
    "             COUNT(*) AS counts\\\n",
    "             FROM Rideshares\\\n",
    "             GROUP BY {0}_col \\\n",
    "             ORDER BY {0}_col ASC\".format(col)\n",
    "    result = hiveContext.sql(query)\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-scores\n",
    "> Z-scores are used to measure an observation's deviation from the group's mean value.\n",
    "Z-scores reveal whether a score is typical for a specified data set or if it is atypical\n",
    "\n",
    "In the below cells we look at two examples of how z-scores can help us catch unusual values.\n",
    "* The first is screening for unusually expensive rides by looking at the **order_total_amount** field.\n",
    "* The second is screening data for unusual ride count based on avg weekly ride count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the order_total_amount for each order and rank them from greatest\n",
    "# Taking the z-score of each order_total_amount can tell us if the value is unusual\n",
    "# From this data we see that there are some rides with unusually high prices that\n",
    "# deviate from the overall avg ride total\n",
    "# NOTE: the avg was not normalized by merchant name so the avg is across all rides regardless of merchant\n",
    "query = \"SELECT merchant_name, \\\n",
    "        order_number, \\\n",
    "        user_id, \\\n",
    "        item_price, \\\n",
    "        order_total_amount, \\\n",
    "        (order_total_amount - AVG(order_total_amount) over()) /stddev(order_total_amount) over() AS ota_zscore \\\n",
    "        FROM Rideshares \\\n",
    "        WHERE order_total_amount IS NOT NULL \\\n",
    "        ORDER BY ota_zscore DESC\"\n",
    "result = hiveContext.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of records with unusual z-scores for order_total_amount, grouped by service\n",
    "# We see that Uber has many more order_total_amounts with z-score above 4 or below -4 \n",
    "query = \"SELECT \\\n",
    "            B.merchant_name, \\\n",
    "            COUNT(B.ota_zscore) \\\n",
    "         FROM \\\n",
    "             (SELECT \\\n",
    "                 merchant_name, \\\n",
    "                 (order_total_amount - AVG(order_total_amount) over()) /stddev(order_total_amount) over() AS ota_zscore\\\n",
    "              FROM Rideshares)B \\\n",
    "         WHERE ota_zscore > 4 OR ota_zscore < -4 \\\n",
    "         GROUP BY merchant_name\"\n",
    "\n",
    "result = hiveContext.sql(query)\n",
    "result.show()\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for Unusual Values in Ride Volume \n",
    "> In this next example we will look at daily rides and compare them to average rides for that given weekday\n",
    "> Ex: we will compare a Monday ride count to average Monday ride count and see if the ride count is unusual for a Monday\n",
    " * data is taken separately for lyft and uber so lyft rides will be compared against lyft averages\n",
    " * Likewise, Uber rides will be compared to Uber averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT \\\n",
    "            B.merchant_name, \\\n",
    "            B.weekday, \\\n",
    "            AVG(B.ride_counts) AS avg_rides\\\n",
    "            FROM( \\\n",
    "             SELECT \\\n",
    "                 date(order_time), \\\n",
    "                 (CASE WHEN DAYOFWEEK(date(order_time)) = 1 THEN 'SUNDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 2 THEN 'MONDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 3 THEN 'TUESDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 4 THEN 'WEDNESDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 5 THEN 'THURSDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 6 THEN 'FRIDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 7 THEN 'SATURDAY' \\\n",
    "                  END) weekday, \\\n",
    "                 merchant_name, \\\n",
    "                 COUNT(*) AS ride_counts \\\n",
    "             FROM Rideshares \\\n",
    "             WHERE date(order_time) IS NOT NULL \\\n",
    "             GROUP BY \\\n",
    "                date(order_time), \\\n",
    "                merchant_name \\\n",
    "            ) B \\\n",
    "        GROUP BY B.merchant_name, B.weekday \\\n",
    "        ORDER BY merchant_name, avg_rides DESC\"\n",
    "rides_per_weekday_result = hiveContext.sql(query)\n",
    "rides_per_weekday_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a new temp table from dataframe\n",
    "rides_per_weekday_result.registerTempTable(\"Rideshares_per_weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rides_per_weekday_result.filter(rides_per_weekday_result.weekday == 'SATURDAY').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many riders there were for each service per day.\n",
    "# an active user is someone who makes at least 2 transactions per quarter\n",
    "query = \"SELECT \\\n",
    "         date(order_time), \\\n",
    "         (CASE WHEN DAYOFWEEK(date(order_time)) = 1 THEN 'SUNDAY' \\\n",
    "               WHEN DAYOFWEEK(date(order_time)) = 2 THEN 'MONDAY' \\\n",
    "               WHEN DAYOFWEEK(date(order_time)) = 3 THEN 'TUESDAY' \\\n",
    "               WHEN DAYOFWEEK(date(order_time)) = 4 THEN 'WEDNESDAY' \\\n",
    "               WHEN DAYOFWEEK(date(order_time)) = 5 THEN 'THURSDAY' \\\n",
    "               WHEN DAYOFWEEK(date(order_time)) = 6 THEN 'FRIDAY' \\\n",
    "               WHEN DAYOFWEEK(date(order_time)) = 7 THEN 'SATURDAY' \\\n",
    "          END) weekday, \\\n",
    "         merchant_name, \\\n",
    "         COUNT(*) AS ride_counts \\\n",
    "         FROM Rideshares \\\n",
    "         WHERE date(order_time) IS NOT NULL \\\n",
    "         GROUP BY date(order_time), merchant_name \\\n",
    "         ORDER BY date(order_time)\"\n",
    "result = hiveContext.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" SELECT \\\n",
    "            B.ride_date, \\\n",
    "            B.weekday, \\\n",
    "            B.merchant_name, \\\n",
    "            B.ride_counts AS actual_ride_count, \\\n",
    "            CAST(C.avg_rides AS INT), \\\n",
    "            CAST((B.ride_counts - C.avg_rides) AS INT) AS rides_diff, \\\n",
    "            (B.ride_counts - C.avg_rides)/ stddev(B.ride_counts) over() AS zscore \\\n",
    "            FROM( \\\n",
    "                SELECT \\\n",
    "                 date(order_time) AS ride_date, \\\n",
    "                 (CASE WHEN DAYOFWEEK(date(order_time)) = 1 THEN 'SUNDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 2 THEN 'MONDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 3 THEN 'TUESDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 4 THEN 'WEDNESDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 5 THEN 'THURSDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 6 THEN 'FRIDAY' \\\n",
    "                       WHEN DAYOFWEEK(date(order_time)) = 7 THEN 'SATURDAY' \\\n",
    "                  END) weekday, \\\n",
    "                 merchant_name, \\\n",
    "                 COUNT(*) AS ride_counts \\\n",
    "                 FROM Rideshares \\\n",
    "                 WHERE date(order_time) IS NOT NULL \\\n",
    "                 GROUP BY date(order_time), merchant_name \\\n",
    "                ) B \\\n",
    "             LEFT JOIN Rideshares_per_Weekday AS C \\\n",
    "                 ON B.weekday = C.weekday AND B.merchant_name = C.merchant_name \\\n",
    "            ORDER BY zscore\"\n",
    "result = hiveContext.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As seen from the table above...\n",
    "There are ride counts that deviate from the mean ride count by several std deviations. For example the first few rows show significantly fewer Uber rides than the usual for a Saturday and Friday. Likewise if we were to order by decreasing z-scores, we might see a surge in rides that is unusual as well. Depending on our tolerance for deviations, these may or may not be outliers. \n",
    "#### One suggestion for use of this data is to look at instances where there is complimentary deviation in Lyft and Uber ride counts. \n",
    "> For example, if Uber rides go down at the same time as Lyft rides go up, this could indicate customer defection from one platform to the other. It would be especially interesting if Uber rides go down and lyft goes up by a similar amount to Uber's lost rides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
